{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EE4708_Assignment2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMJx2VqIWQiRkp9HYXKiRG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swathi1309/Data_Analytics/blob/main/EE4708_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs9GL8wEXoES"
      },
      "source": [
        "# Logistic Regression for Predicting Survival using the Titanic Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9r2Xhi7XvpN"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqwsLKKoH4Mz"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import re\n",
        "\n",
        "import missingno as msno\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnF5_HjcXyZH"
      },
      "source": [
        "## Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxl5lADSKvdg"
      },
      "source": [
        "data = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Data Cleaning\n",
        "# Dropping columns that are not relevant\n",
        "to_drop = ['Name', 'Ticket', 'PassengerId']\n",
        "data.drop(columns=to_drop, inplace=True)\n",
        "# Dropping rows where embarked is not available\n",
        "data.dropna(subset=['Embarked'], inplace=True)\n",
        "# Converting categorical variables to one-hot encodings\n",
        "sex = pd.get_dummies(data[\"Sex\"])\n",
        "embarked = pd.get_dummies(data[\"Embarked\"])\n",
        "data = pd.concat([data,sex,embarked],axis=1)"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WbvB1ZKYG6v"
      },
      "source": [
        "# Data imputation for Age feature\n",
        "sb.displot(data, x='Age', hue='Pclass', kind='kde', col='Sex', palette='flare');\n",
        "data['Age'] = data.groupby(['Pclass'])['Age'].apply(lambda x:x.fillna(x.median()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-SJ0eyXYI8U"
      },
      "source": [
        "# Creating categorical variables from continuous variables, and creating one-hot encodings for the same\n",
        "sb.displot(data, x=\"Age\", palette='flare');\n",
        "sb.displot(data, x=\"Fare\", palette='flare');\n",
        "\n",
        "data['Age_bins'] = pd.cut(data['Age'], bins=[data['Age'].min()-1,5,18,25,50,data['Age'].max()+1], labels=['age <5', 'age 5-18', 'age 18-25', 'age 25-50','age >50'])\n",
        "data['Fare_bins'] = pd.cut(data['Fare'], bins=[data['Fare'].min()-1, 10, 20, 30, 100, data['Fare'].max()+1], labels=['fare <10', 'fare 10-20','fare 20-30','fare 30-100','fare >100'])\n",
        "\n",
        "sb.displot(data, x=\"Age_bins\", palette='flare');\n",
        "sb.displot(data, x=\"Fare_bins\", palette='flare');\n",
        "\n",
        "age_bins = pd.get_dummies(data['Age_bins'])\n",
        "fare_bins = pd.get_dummies(data['Fare_bins'])\n",
        "data = pd.concat([data, age_bins, fare_bins], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9dJoN73Y6_q"
      },
      "source": [
        "normalized_age =(data['Age']-data['Age'].min())/(data['Age'].max()-data['Age'].min())\n",
        "data['Age'] = normalized_age\n",
        "\n",
        "normalized_fare = (data['Fare']-data['Fare'].min())/(data['Fare'].max()-data['Fare'].min())\n",
        "data['Fare'] = normalized_fare"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY4cTIDkYRu5"
      },
      "source": [
        "## Data Visualization and Exploratory Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQLeIp7Ugm7E"
      },
      "source": [
        "sb.catplot(x=\"Pclass\", y=\"Survived\", hue='Sex', data=data, saturation=.5, kind=\"bar\", palette='pastel');\n",
        "\n",
        "sb.displot(data=data, x='Age', kde=True, col='Survived');\n",
        "sb.displot(data=data, x='Fare_bins', hue='Survived');\n",
        "\n",
        "data['Cabin'] = data['Cabin'].apply(lambda s:s[0] if pd.notnull(s) else s)\n",
        "sb.catplot(x='Cabin', y='Survived', data=data, kind='bar', order = ['A','B','C','D','E','F','G','T']);\n",
        "\n",
        "sb.catplot(x=\"SibSp\", y=\"Survived\",data=data, kind=\"bar\");\n",
        "sb.catplot(x=\"Parch\", y=\"Survived\",data=data, kind=\"bar\");\n",
        "sb.catplot(x=\"Embarked\", y=\"Survived\",data=data, kind=\"bar\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWRC2mmUjWJy"
      },
      "source": [
        "## Building and Comparing Regression Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf92yZzSjVk9"
      },
      "source": [
        "data.drop(columns=['Cabin'], inplace=True)\n",
        "X_continous = data[['Pclass', 'male', 'Age', 'SibSp', 'Parch', 'Fare', 'C', 'Q']]\n",
        "X_classes = data[['Pclass', 'male', 'age <5', 'age 5-18', 'age 18-25', 'age 25-50', 'fare <10', 'fare 10-20', 'fare 20-30', 'fare 30-100', 'SibSp', 'Parch', 'C', 'Q']]\n",
        "Y = data['Survived']"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdNhWt1kx3kl"
      },
      "source": [
        "def print_coeff(model, features, scores):\n",
        "  print (model, \": \\n\")\n",
        "  for i in range(len(features)):\n",
        "    print (features[i], \" :\", scores[0, i])"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsQ1xlUykvd-"
      },
      "source": [
        "# Model 1 - Using continous values for age and fare\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_continous, Y, test_size=0.3, random_state=10)\n",
        "model1 = LogisticRegression(max_iter=5000)\n",
        "model1.fit(x_train, y_train)\n",
        "print_coeff(\"Model1\", ['Pclass', 'male', 'Age', 'SibSp', 'Parch', 'Fare', 'C', 'Q'], model1.coef_)\n",
        "# For training data\n",
        "y_pred = model1.predict(x_train)\n",
        "print (\"Training data:\")\n",
        "print (\"Confusion matrix:\")\n",
        "print (confusion_matrix(y_pred, y_train))\n",
        "print (\"Accuracy: \", accuracy_score(y_pred, y_train))\n",
        "print (\"Precision: \", precision_score(y_pred, y_train))\n",
        "print (\"Recall: \", recall_score(y_pred, y_train))\n",
        "print (\"F1_score: \", f1_score(y_pred, y_train))\n",
        "print (\"\\n\")\n",
        "# For validation data\n",
        "y_pred = model1.predict(x_test)\n",
        "print (\"Validation data:\")\n",
        "print (\"Confusion matrix:\")\n",
        "print (confusion_matrix(y_pred, y_test))\n",
        "print (\"Accuracy: \", accuracy_score(y_pred, y_test))\n",
        "print (\"Precision: \", precision_score(y_pred, y_test))\n",
        "print (\"Recall: \", recall_score(y_pred, y_test))\n",
        "print (\"F1_score: \", f1_score(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZbIwRdDnIZP"
      },
      "source": [
        "# Model 2 - Using binned values for age and fare\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_classes, Y, test_size=0.3, random_state=10)\n",
        "model2 = LogisticRegression(max_iter=5000)\n",
        "model2.fit(x_train, y_train)\n",
        "\n",
        "print (\"Model 2\")\n",
        "print (\"\\n\")\n",
        "# For training data\n",
        "y_pred = model2.predict(x_train)\n",
        "print (\"Training data:\")\n",
        "print (\"Confusion matrix:\")\n",
        "print (confusion_matrix(y_pred, y_train))\n",
        "print (\"Accuracy: \", accuracy_score(y_pred, y_train))\n",
        "print (\"Precision: \", precision_score(y_pred, y_train))\n",
        "print (\"Recall: \", recall_score(y_pred, y_train))\n",
        "print (\"F1_score: \", f1_score(y_pred, y_train))\n",
        "print (\"\\n\")\n",
        "# For validation data\n",
        "y_pred = model2.predict(x_test)\n",
        "print (\"Validation data:\")\n",
        "print (\"Confusion matrix:\")\n",
        "print (confusion_matrix(y_pred, y_test))\n",
        "print (\"Accuracy: \", accuracy_score(y_pred, y_test))\n",
        "print (\"Precision: \", precision_score(y_pred, y_test))\n",
        "print (\"Recall: \", recall_score(y_pred, y_test))\n",
        "print (\"F1_score: \", f1_score(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLyQFu8Y0i4U"
      },
      "source": [
        "# Predicting Survival from the Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUO4QKvE03zy"
      },
      "source": [
        "test_data = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Data cleaning as done for training data\n",
        "to_drop = ['Name', 'Ticket', 'PassengerId']\n",
        "test_data.drop(columns=to_drop, inplace=True)\n",
        "\n",
        "test_data['Age'] = test_data.groupby(['Pclass'])['Age'].apply(lambda x:x.fillna(x.median()))\n",
        "test_data['Fare'] = test_data.groupby(['Pclass'])['Fare'].apply(lambda x:x.fillna(x.median()))\n",
        "\n",
        "sex = pd.get_dummies(test_data[\"Sex\"])\n",
        "embarked = pd.get_dummies(test_data[\"Embarked\"])\n",
        "test_data = pd.concat([test_data,sex,embarked],axis=1)\n",
        "\n",
        "normalized_age =(test_data['Age']-test_data['Age'].min())/(test_data['Age'].max()-test_data['Age'].min())\n",
        "test_data['Age'] = normalized_age\n",
        "\n",
        "normalized_fare = (test_data['Fare']-test_data['Fare'].min())/(test_data['Fare'].max()-test_data['Fare'].min())\n",
        "test_data['Fare'] = normalized_fare\n",
        "\n",
        "test_data.drop(columns=['Cabin'], inplace=True)\n",
        "test_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpxrPnuU2Diw"
      },
      "source": [
        "x = test_data[['Pclass', 'male', 'Age', 'SibSp', 'Parch', 'Fare', 'C', 'Q']]\n",
        "y = model1.predict(x)\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "survived = pd.DataFrame(y)\n",
        "survived.columns = [\"Survived\"]\n",
        "predictions = pd.concat([test, survived], axis=1)\n",
        "predictions.to_csv('predictions.csv')\n",
        "survived[\"Survived\"].value_counts()"
      ],
      "execution_count": 214,
      "outputs": []
    }
  ]
}
